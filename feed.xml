<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hchlhwang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hchlhwang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-28T04:34:07+00:00</updated><id>https://hchlhwang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">CYBATHLON Challenges 2023 Results</title><link href="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-3/" rel="alternate" type="text/html" title="CYBATHLON Challenges 2023 Results"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://hchlhwang.github.io/blog/2023/Cybathlon%20Challenges%202023%203</id><content type="html" xml:base="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-3/"><![CDATA[<h2 id="vision-assistive-race-results">Vision Assistive Race Results</h2> <p>Our FlashLight team was placed in 2nd place for the Vision Assistive Race. Although we were close to completing the first task, unfortunately, we were not able finish a single task as shown below. The light reflection issue was a challenge for task 1 and our implementation for solving task 2 was not fully integrated.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog/cybathlon/results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VIS race results </div> <p>You can watch the full video from the <a href="https://cybathlon.ethz.ch/en/events/challenges/Challenges-2023">official link</a>.</p> <hr/> <h2 id="takeaways">Takeaways</h2> <p>Solving a problem for a competition is different from solving a real-world problem. In a competition, we could use heuristical methods and rely on the ability of the pilot to solve problems. However, such method would be limited when deployed in real-world scenarios.</p> <p>Simple is best. Our system was complex compared to a single mobuile device as our system contained various sensors and motors to provide feedback to the pilot. The hardware and software of our system was not optimized at the time of the competition.</p> <p>Our pilots mentioned that the second task was not relevant to the real-world as they usually did not serve food.</p> <hr/> <h2 id="cybathlon-2024">CYBATHLON 2024</h2> <p>It was meaningful to join the CYBATHLON Challneges 2023 Vison Assistance Race (VIS). It was the first time to have the VIS race. I can know see that there are multiple groups joining the CYBATHLON 2024. I believe that CYBATHLON can push the advancements in assistive technologies aiming for real-world deployment.</p> <iframe width="640" height="360" src="https://www.youtube.com/embed/WwXPCI1apZk"> </iframe> ]]></content><author><name>Hochul Hwang</name></author><category term="cybathlon"/><category term="human-computer-interaction"/><category term="computer-vision"/><category term="object-detection"/><summary type="html"><![CDATA[2nd place]]></summary></entry><entry><title type="html">Preparation of the CYBATHLON Challenges 2023 (2)</title><link href="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-2/" rel="alternate" type="text/html" title="Preparation of the CYBATHLON Challenges 2023 (2)"/><published>2023-04-04T00:00:00+00:00</published><updated>2023-04-04T00:00:00+00:00</updated><id>https://hchlhwang.github.io/blog/2023/Cybathlon%20Challenges%202023%202</id><content type="html" xml:base="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-2/"><![CDATA[<h2 id="task-2-serving-food">Task 2. Serving Food</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog/cybathlon/task2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Task 2: Serving food </div> <p>The aim of this task is to 1) pour water to a designated level, 2) carry &amp; balance a tray having a bowl and glass filled with water, 3) move to the second table on the side where there are no utensils, 4) and place the bowl and glass without touching the utensils.</p> <p>It is critical to understand the scene and to provide proper commands to the pilot to complete the subtasks in task 2. To understand the scene, vision-based deep learning models were utilized and vibration signals were implmented to provide feedback to the pilot.</p> <hr/> <h2 id="pouring-water">Pouring water</h2> <p>We utilized a scale to measure the desired amound of water poured into the cup and when there are enough water poured, the vibration motors attached to the pilotâ€™s stopped vibratiing.</p> <hr/> <h2 id="deciding-the-target-location">Deciding the target location</h2> <p>The objective of this subtask is to move to the target location where there are no utensils on the table. First, to determine the correct direction (right or left), <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a> is utilized to extract the bounding box of the dining table. Then, I compared the center $x$ position of the dining table and the $x$ position of the averaged red pixels and decide the desired direction as the logic below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="k">if</span> <span class="p">(</span><span class="n">tableX</span> <span class="o">&gt;</span> <span class="n">redX</span><span class="p">){</span>
  <span class="c1">// Utensil is on left of the table</span>
  <span class="c1">// Move to right </span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
  <span class="c1">// Utensil is on right of the table</span>
  <span class="c1">// Move to left</span>
<span class="p">}</span> 
</pre></td></tr></tbody></table></code></pre></figure> <p>When the desired direction is determined, the red object detection function (Python) publishes an <a href="http://lcm-proj.github.io/lcm/">LCM</a> message to the navigation code (C++). The following video indicates the desired direction on the top left corner in red text while the bounding box of the dining table is in blue and average position of the red objects are in green.</p> <iframe width="640" height="360" src="https://www.youtube.com/embed/h8slOW101kk"> </iframe> <hr/> <h2 id="balancing-the-tray">Balancing the tray</h2> <p>Task 1: Sidewalk. The aim is to reach the end line without bumping into any obstacles.</p> <hr/> <h2 id="placing-bowl-and-glass">Placing bowl and glass</h2> <p>I finetuned <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a> on our dataset collected in various viewpoints, light settings, and backgrounds. Based on the bounding boxes obtained during inference, the desired direction of placing the bowl was determined. This direction was then conveyed to the pilot via haptic feedback via vibration motors.</p> <iframe width="640" height="360" src="https://www.youtube.com/embed/bjo3fgSEHGo"> </iframe> <hr/> <h2 id="thanks">Thanks</h2> <p>Thankfully, the recruitment process was relatively smooth. We have two incredible pilots (with great atheletic abilities) and one guide dog trainer. We also have great faculty members joining. Finally, we have wonderful members for developing hardware, software, and conducting experiments. Always thankful for everyone. Especially, thankful for my advisor for the support.</p> ]]></content><author><name>Hochul Hwang</name></author><category term="cybathlon"/><category term="human-computer-interaction"/><category term="computer-vision"/><category term="object-detection"/><summary type="html"><![CDATA[Development components for task 2]]></summary></entry><entry><title type="html">Preparation of the CYBATHLON Challenges 2023 (1)</title><link href="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-1/" rel="alternate" type="text/html" title="Preparation of the CYBATHLON Challenges 2023 (1)"/><published>2023-03-09T00:00:00+00:00</published><updated>2023-03-09T00:00:00+00:00</updated><id>https://hchlhwang.github.io/blog/2023/Cybathlon%20Challenges%202023%201</id><content type="html" xml:base="https://hchlhwang.github.io/blog/2023/Cybathlon-Challenges-2023-1/"><![CDATA[<h2 id="cybathlon-challenges-2023">Cybathlon Challenges 2023</h2> <p><a href="https://cybathlon.ethz.ch/en">CYBATHLON</a>, a project of ETH Zurich, acts as a platform that challenges teams from all over the world to develop assistive technologies suitable for everyday use with and for people with disabilities. The driving force behind CYBATHLON is international competitions and events, in which teams consisting of technology developers from universities, companies or NGOs and a person with disabilities (pilot) tackle various everyday tasks with their latest assistive technologies.</p> <p>CYBATHLON is held every four years since 2016. It will be held next year, 2024. This year they are organizing the CYBATHLON Challeneges having two tasks for each discipline.</p> <hr/> <h2 id="vision-assistive-race">Vision Assistive Race</h2> <p>It is the first year for CYBATHLON to have a race considering the blind or visually impaired individuals.</p> <hr/> <h2 id="tasks">Tasks</h2> <p>Task 1: Sidewalk. The aim is to reach the end line without bumping into any obstacles.</p> <p>Task 2: Serving food. The aim is to 1) pour water to a designated level, 2) carry &amp; balance a tray having a bowl and glass filled with water, 3) placing the bowl and glass without touching the utensils that are already set on the table.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog/cybathlon/task1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog/cybathlon/task2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Two tasks for the Vision Assistive Race in CYBATHLON Challenges 2023. </div> <hr/> <h2 id="team">Team</h2> <p>Thankfully, the recruitment process was relatively smooth. We have two incredible pilots (with great atheletic abilities) and one guide dog trainer. We also have great faculty members joining. Finally, we have wonderful members for developing hardware, software, and conducting experiments. Always thankful for everyone. Especially, thankful for my advisor for the support.</p> <p>Our team name is <a href="https://cybathlon.ethz.ch/en/teams/flash-light">FlashLight</a>. We aim to light up the path for the blind or visually impaired individuals to quickly accomplish tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/blog/cybathlon/team.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> FlashLight team! Some of the members are missing. </div> ]]></content><author><name>Hochul Hwang</name></author><category term="cybathlon"/><category term="human-computer-interaction"/><summary type="html"><![CDATA[Introduction of the Vision Assistance Race tasks]]></summary></entry></feed>