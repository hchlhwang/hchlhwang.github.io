<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Hochul Hwang</title> <meta name="author" content="Hochul Hwang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hchlhwang.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Projects</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <script src="/assets/js/walker-animation.js" defer></script> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hochul</span> Hwang <span class="header-links"> <a href="/assets/pdf/resume-hochul.pdf" target="_blank">cv</a> <a href="https://scholar.google.com/citations?user=CLSqawUAAAAJ&amp;hl" target="_blank" rel="external nofollow noopener">scholar</a> </span> </h1> <p class="desc"></p> <div class="physics-tagline"> <span class="walker">ü¶ø</span> Building robots for independent mobility for all.</div> </header> <article> <div class="profile float-left"> <figure> <picture> <img src="/assets/img/new-hochul.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="new-hochul.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a Ph.D. candidate at UMass Amherst developing <strong>guide dog robots</strong> for visually impaired individuals. My research spans <strong>robot perception</strong>, <strong>planning</strong>, and <strong>human-robot interaction</strong>.</p> <p><strong>Perception &amp; Planning</strong> ‚Äî Building vision-based navigation systems using foundation models for safe, long-term autonomy.</p> <p><strong>Human-Robot Interaction</strong> ‚Äî Investigating how users interact with assistive robots through field studies, informing design for real-world deployment.</p> <p>Advised by <a href="https://www.cics.umass.edu/people/kim-donghyun" rel="external nofollow noopener" target="_blank">Donghyun Kim</a>, co-advised by <a href="https://www.umass.edu/ials/people/sunghoon-ivan-lee" rel="external nofollow noopener" target="_blank">Ivan Lee</a> and <a href="https://www.cs.utexas.edu/people/faculty-researchers/joydeep-biswas" rel="external nofollow noopener" target="_blank">Joydeep Biswas</a>.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 31, 2026</th> <td> Two papers (<a href="https://arxiv.org/abs/2505.11808" rel="external nofollow noopener" target="_blank">Quiet Locomotion Control</a> and GuideTWSI) submitted to ICRA‚Äô26 got accepted! </td> </tr> <tr> <th scope="row">Dec 8, 2025</th> <td> Our GuideNav: vision-only visual teach-and-repeat paper submitted to <a href="https://arxiv.org/abs/2512.06147" rel="external nofollow noopener" target="_blank">HRI‚Äô26</a> got accepted! </td> </tr> <tr> <th scope="row">Sep 30, 2025</th> <td> I started my internship at <a href="https://glidance.io/" rel="external nofollow noopener" target="_blank">Glidance</a> as a Robotics &amp; ML Intern! </td> </tr> <tr> <th scope="row">Jun 27, 2024</th> <td> üèÖ Our <a href="https://arxiv.org/abs/2402.06794" rel="external nofollow noopener" target="_blank">UR‚Äô24</a> paper has been nominated as <span style="color:#DC2626; font-weight:600;">Finalist</span> (top 9 papers of all submissions)! </td> </tr> <tr> <th scope="row">Apr 27, 2024</th> <td> üèÜ Our <a href="https://dl.acm.org/doi/10.1145/3613904.3642181" rel="external nofollow noopener" target="_blank">CHI‚Äô24</a> paper has won the <span style="color:#DC2626; font-weight:600;">Best Paper Award</span> (top 1% of all submissions)! </td> </tr> </table> </div> </div> <div class="publications"> <h2>publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hri-final.gif"></div> <div id="hwang2026guidenav" class="col-sm-8"> <div class="title">GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers<a href="https://arxiv.org/abs/2512.06147" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Soowan Yang,¬†Jahir S Monon, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Nicholas A Giudice, Sunghoon I Lee, Joydeep Biswas, Donghyun Kim' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>HRI</em> 2026 </div> <div class="links"> </div> <div class="abstract hidden"> <p>While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions‚Äîall without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system‚Äôs feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hwang2026guidenav</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hwang, Hochul and Yang, Soowan and Monon, Jahir S and Giudice, Nicholas A and Lee, Sunghoon I and Biswas, Joydeep and Kim, Donghyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{HRI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/locomotion25.gif"></div> <div id="yu2026locomotion" class="col-sm-8"> <div class="title">Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control<a href="https://arxiv.org/abs/2505.11808" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a><a href="https://youtu.be/8-pz_8Hqe6s" title="Video" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-video fa-sm"></i></a> </div> <div class="author"> Shangqun Yu,¬†<em>Hochul Hwang</em>,¬†Trung M Dang, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Joydeep Biswas, Nicholas A Giudice, Sunghoon Ivan Lee, Donghyun Kim' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>ICRA</em> 2026 </div> <div class="links"> </div> <div class="abstract hidden"> <p>A quadruped robot is a promising system that can offer assistance comparable to that of dog guides due to its similar form factor. However, various challenges remain in making these robots a reliable option for blind and low-vision (BLV) individuals. Among these challenges, noise and jerky motion during walking are critical drawbacks of existing quadruped robots. While these issues have largely been overlooked in guide dog robot research, our interviews with guide dog handlers and trainers revealed that acoustic and physical disturbances can be particularly disruptive for BLV individuals, who rely heavily on environmental sounds for navigation. To address these issues, we developed a novel walking controller for slow stepping and smooth foot swing/contact while maintaining human walking speed, as well as robust and stable balance control. The controller integrates with a perception system to facilitate locomotion over non-flat terrains, such as stairs. Our controller was extensively tested on the Unitree Go1 robot and, when compared with other control methods, demonstrated significant noise reduction ‚Äì half of the default locomotion controller. In this study, we adopt a mixed-methods approach to evaluate its usability with BLV individuals. In our indoor walking experiments, participants compared our controller to the robot‚Äôs default controller. Results demonstrated superior acceptance of our controller, highlighting its potential to improve the user experience of guide dog robots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2026locomotion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Shangqun and Hwang, Hochul and Dang, Trung M and Biswas, Joydeep and Giudice, Nicholas A and Lee, Sunghoon Ivan and Kim, Donghyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://youtu.be/8-pz_8Hqe6s}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ava25.png"></div> <div id="hwang2025ava" class="col-sm-8"> <div class="title">AVA in Action: Developing a Guide Dog Robot for Blind and Low-Vision People</div> <div class="author"> <em>Hochul Hwang</em>,¬†Krisha Adhikari,¬†Parth Goel, and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Anh Nguyen, Satya Shodhaka, Shangqun Yu, Trung M Dang, Ken Suzuki, Georges Chebly, Peter White, Joydeep Biswas, Nicholas A Giudice, Sunghoon I Lee, Donghyun Kim' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> <em>In CVPR Workshop on AVA</em> 2025 </div> <div class="links"> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hwang2025ava</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AVA in Action: Developing a Guide Dog Robot for Blind and Low-Vision People}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hwang, Hochul and Adhikari, Krisha and Goel, Parth and Nguyen, Anh and Shodhaka, Satya and Yu, Shangqun and Dang, Trung M and Suzuki, Ken and Chebly, Georges and White, Peter and Biswas, Joydeep and Giudice, Nicholas A and Lee, Sunghoon I and Kim, Donghyun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR Workshop on AVA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/assets24-w.png"></div> <div id="hwang2024urban" class="col-sm-8"> <div class="title">Lessons Learned from Developing a Human-Centered Guide Dog Robot for Mobility Assistance<a href="https://www.arxiv.org/abs/2409.19778" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Ken Suzuki,¬†Nicholas A Giudice, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Joydeep Biswas, Sunghoon I Lee, Donghyun Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ASSETS UrbanAccess Workshop</em> 2024 </div> <div class="links"> </div> <div class="abstract hidden"> <p>While guide dogs offer essential mobility assistance, their high cost, limited availability, and care requirements make them inaccessible to most blind or low vision (BLV) individuals. Recent advances in quadruped robots provide a scalable solution for mobility assistance, but many current designs fail to meet real-world needs due to a lack of understanding of handler and guide dog interactions. In this paper, we share lessons learned from developing a human-centered guide dog robot, addressing challenges such as optimal hardware design, robust navigation, and informative scene description for user adoption. By conducting semi-structured interviews and hu- man experiments with BLV individuals, guide-dog handlers, and trainers, we identified key design principles to improve safety, trust, and usability in robotic mobility aids. Our findings lay the building blocks for future development of guide dog robots, ultimately enhancing independence and quality of life for BLV individuals.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/rita24.png"></div> <div id="hwang2024rita" class="col-sm-8"> <div class="title">Synthetic data augmentation for robotic mobility aids to support blind and low vision people<a href="https://arxiv.org/abs/2409.11164" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Krisha Adhikari,¬†Satya Shodhaka, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Donghyun Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>RiTA</em> 2024 </div> <div class="links"> </div> <div class="abstract hidden"> <p>Robotic mobility aids for blind and low-vision (BLV) individuals rely heavily on deep learning-based vision models specialized for various navigational tasks. However, the performance of these models is often constrained by the availability and diversity of real-world datasets, which are challenging to collect in sufficient quantities for different tasks. In this study, we investigate the effectiveness of synthetic data, generated using Unreal Engine 4, for training robust vision models for this safety-critical application. Our findings demonstrate that synthetic data can enhance model performance across multiple tasks, showcasing both its potential and its limitations when compared to real-world data. We offer valuable insights into optimizing synthetic data generation for developing robotic mobility aids. Additionally, we publicly release our generated synthetic dataset to support ongoing research in assistive technologies for BLV individuals, available at https://hchlhwang.github.io/SToP.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ur24.png"></div> <div id="hwang2024cross" class="col-sm-8"> <div class="title"> <span class="badge award-badge" title="Best Paper Finalist">üèÜ Best Paper Finalist</span>Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing<a href="https://ieeexplore.ieee.org/document/10597464" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Sunjae Kwon,¬†Yekyung Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Donghyun Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>UR</em> 2024 </div> <div class="links"> </div> <div class="abstract hidden"> <p>Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context ‚Äì a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual knowledge, extracted from images, and text prompt, we evaluate a large multimodal model for safety score prediction and scene description. Our findings highlight the reasoning and safety score prediction capabilities of a LMM, activated by various prompts, as a pathway to developing a trustworthy system, crucial for applications requiring reliable decision-making support.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/chi24.png"></div> <div id="hwang2024gd" class="col-sm-8"> <div class="title"> <span class="badge award-badge" title="Best Paper">üèÜ Best Paper</span>Towards Robotic Companions: Understanding Handler-Guide Dog Interactions for Informed Guide Dog Robot Design<a href="https://arxiv.org/html/2402.06790v1" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a><a href="https://www.youtube.com/watch?v=Tb81bPVinoY" title="Video" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-video fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Hee-Tae Jung,¬†Nicholas A Giudice, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Joydeep Biswas, Sunghoon I Lee, Donghyun Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>CHI</em> 2024 </div> <div class="links"> </div> <div class="abstract hidden"> <p>Dog guides are favored by blind and low-vision (BLV) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. However, only a relatively small proportion of BLV people work with dog guides due to their limited availability and associated maintenance responsibilities. There is considerable recent interest in addressing this challenge by developing legged guide dog robots. This study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. We conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. Thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. Grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the BLV community.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/icra23.gif"></div> <div id="hwang2023system" class="col-sm-8"> <div class="title">System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work<a href="https://ieeexplore.ieee.org/document/10160573" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a><a href="https://www.youtube.com/watch?v=9Y7Gvbw0qr4" title="Video" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-video fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Tim Xia,¬†Ibrahima Keita, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ken Suzuki, Joydeep Biswas, Sunghoon I Lee, Donghyun Kim' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>ICRA</em> 2023 </div> <div class="links"> </div> <div class="abstract hidden"> <p>A robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, potential for mass production, and low maintenance burden. However, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. To develop a robotic guiding system that is genuinely beneficial to blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers and trainers and first-hand blindfold walking experiences with various guide dogs. Grounded on the facts learned from vivid experience and interviews, we build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. For collaborative navigation, we propose a semantic- aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler‚Äôs position and directional cues to determine the collision-free path. We evaluate our integrated robotic system by testing guide blindfold walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed (0.7 m/s).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hwang2023system</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hwang, Hochul and Xia, Tim and Keita, Ibrahima and Suzuki, Ken and Biswas, Joydeep and Lee, Sunghoon I and Kim, Donghyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=9Y7Gvbw0qr4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iros-w.gif"></div> <div id="zhu2023dynamic" class="col-sm-8"> <div class="title">Dynamic Object Avoidance using Event-Data for a Quadruped Robot<a href="https://ippc-iros23.github.io/papers/zhu.pdf" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a><a href="https://www.youtube.com/watch?v=wEPvynkVlLA" title="Video" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-video fa-sm"></i></a> </div> <div class="author"> Shifan Zhu,¬†Nisal Perera,¬†Shangqun Yu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Hochul Hwang, Donghyun Kim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IROS IPPC Workshop</em> 2023 </div> <div class="links"> </div> <div class="abstract hidden"> <p>As robots increase in agility and encounter fast- moving objects, dynamic object detection and avoidance become notably challenging. Traditional RGB cameras, burdened by motion blur and high latency, often act as the bottleneck. Event cameras have recently emerged as a promising solution for the challenges related to rapid movement. In this paper, we introduce a dynamic object avoidance framework that integrates both event and RGBD cameras. Specifically, this framework first estimates and compensates for the event‚Äôs motion to detect dynamic objects. Subsequently, depth data is combined to derive a 3D trajectory. When initiating from a static state, the robot adjusts its height based on the predicted collision point to avoid the dynamic obstacle. Through real-world experiments with the Mini-Cheetah, our approach successfully circumvents dynamic objects at speeds up to 5 m/s, achieving an 83% success rate. Supplemental video: </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2023dynamic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Object Avoidance using Event-Data for a Quadruped Robot}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Shifan and Perera, Nisal and Yu, Shangqun and Hwang, Hochul and Kim, Donghyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IROS IPPC Workshop}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=wEPvynkVlLA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/advmat21.png"></div> <div id="ha2021highly" class="col-sm-8"> <div class="title">Highly sensitive capacitive pressure sensors over a wide pressure range enabled by the hybrid responses of a highly porous nanocomposite<a href="https://advanced.onlinelibrary.wiley.com/doi/epdf/10.1002/adma.202103320" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> Kyoung-Ho Ha,¬†Weiyi Zhang,¬†Hongwoo Jang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Seungmin Kang, Liu Wang, Philip Tan, Hochul Hwang, Nanshu Lu' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Advanced Materials</em> 2021 </div> <div class="links"> </div> <div class="abstract hidden"> <p>Past research aimed at increasing the sensitivity of capacitive pressure sensors has mostly focused on developing dielectric layers with surface/porous structures or higher dielectric constants. However, such strategies have only been effective in improving sensitivities at low pressure ranges (e.g., up to 3 kPa). To overcome this well-known obstacle, herein, a flexible hybrid-response pressure sensor (HRPS) composed of an electrically conductive porous nanocomposite (PNC) laminated with an ultrathin dielectric layer is devised. Using a nickel foam template, the PNC is fabricated with carbon nanotubes (CNTs)-doped Ecoflex to be 86% porous and electrically conductive. The PNC exhibits hybrid piezoresistive and piezocapacitive responses, resulting in significantly enhanced sensitivities (i.e., more than 400%) over wide pressure ranges, from 3.13 kPa‚àí1 within 0‚Äì1 kPa to 0.43 kPa‚àí1 within 30‚Äì50 kPa. The effect of the hybrid responses is differentiated from the effect of porosity or high dielectric constants by comparing the HRPS with its purely piezocapacitive counterparts. Fundamental understanding of the HRPS and the prediction of optimal CNT doping are achieved through simplified analytical models. The HRPS is able to measure pressures from as subtle as the temporal arterial pulse to as large as footsteps.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ha2021highly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Highly sensitive capacitive pressure sensors over a wide pressure range enabled by the hybrid responses of a highly porous nanocomposite}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ha, Kyoung-Ho and Zhang, Weiyi and Jang, Hongwoo and Kang, Seungmin and Wang, Liu and Tan, Philip and Hwang, Hochul and Lu, Nanshu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advanced Materials}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/access21.gif"></div> <div id="9324837" class="col-sm-8"> <div class="title">ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications<a href="https://ieeexplore.ieee.org/document/9324837" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> <em>Hochul Hwang</em>,¬†Cheongjae Jang,¬†Geonwoo Park, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Junghyun Cho, Ig-Jae Kim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IEEE Access</em> 2021 </div> <div class="links"> </div> <div class="abstract hidden"> <p>To train deep learning models for vision-based action recognition of elders‚Äô daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders‚Äô activities in many aspects, making it challenging to recognize elders‚Äô daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders‚Äô daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders‚Äô activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of-the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders‚Äô daily activities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9324837</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hwang, Hochul and Jang, Cheongjae and Park, Geonwoo and Cho, Junghyun and Kim, Ig-Jae}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ACCESS.2021.3051842}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9324837}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/iros18-2.gif"></div> <div id="kim2018computationally" class="col-sm-8"> <div class="title">Computationally-robust and efficient prioritized whole-body controller with contact constraints<a href="https://ieeexplore.ieee.org/document/8593767" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a><a href="https://www.youtube.com/watch?v=3uc_p-6tzLg" title="Video" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-video fa-sm"></i></a> </div> <div class="author"> Donghyun Kim,¬†Jaemin Lee,¬†Junhyeok Ahn, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Orion Campbell, Hochul Hwang, Luis Sentis' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>IROS</em> 2018 </div> <div class="links"> </div> <div class="abstract hidden"> <p>In this paper, we devise methods for the multiobjective control of humanoid robots, a.k.a. prioritized whole-body controllers, that achieve efficiency and robustness in the algorithmic computations. We use a form of whole-body controllers that is very general via incorporating centroidal momentum dynamics, operational task priorities, contact reaction forces, and internal force constraints. First, we achieve efficiency by solving a quadratic program that only involves the floating base dynamics and the reaction forces. Second, we achieve computational robustness by relaxing task accelerations such that they comply with friction cone constraints. Finally, we incorporate methods for smooth contact transitions to enhance the control of dynamic locomotion behaviors. The proposed methods are demonstrated both in simulation and in real experiments using a passive-ankle bipedal robot.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2018computationally</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Computationally-robust and efficient prioritized whole-body controller with contact constraints}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Donghyun and Lee, Jaemin and Ahn, Junhyeok and Campbell, Orion and Hwang, Hochul and Sentis, Luis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=3uc_p-6tzLg}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/humanoids18.gif"></div> <div id="kim2018control" class="col-sm-8"> <div class="title">Control scheme and uncertainty considerations for dynamic balancing of passive-ankled bipeds and full humanoids<a href="https://ieeexplore.ieee.org/document/8624915" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> Donghyun Kim,¬†Steven Jens Jorgensen,¬†<em>Hochul Hwang</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Luis Sentis' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Humanoids</em> 2018 </div> <div class="links"> </div> <div class="abstract hidden"> <p>We propose a methodology for dynamically balancing passive-ankled bipeds and full humanoids. As dynamic locomotion without ankle-actuation is more difficult than with actuated feet, our control scheme adopts an efficient whole-body controller that combines inverse kinematics, contact-consistent feed-forward torques, and low-level motor position controllers. To understand real-world sensing and controller requirements, we perform an uncertainty analysis on the linear-inverted-pendulum (LIP)-based footstep planner. This enables us to identify necessary hardware and control refinements to demonstrate that our controller can achieve long-term unsupported dynamic balancing on our series-elastic biped, Mercury. Through simulations, we also demonstrate that our control scheme for dynamic balancing with passive-ankles is applicable to full humanoid robots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2018control</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Control scheme and uncertainty considerations for dynamic balancing of passive-ankled bipeds and full humanoids}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Donghyun and Jorgensen, Steven Jens and Hwang, Hochul and Sentis, Luis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Humanoids}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/patent24.png"></div> <div id="cho2024human" class="col-sm-8"> <div class="title">Human behavior recognition system and method using hierachical class learning considering safety<a href="https://patents.google.com/patent/US20220207920A1/en" title="Paper" style="margin-left: 0.3em;" rel="external nofollow noopener" target="_blank"><i class="fas fa-file-alt fa-sm"></i></a> </div> <div class="author"> Junghyun Cho,¬†Ig Jae Kim,¬†and¬†<em>Hochul Hwang</em> </div> <div class="periodical"> <em>US Patent</em> 2024 </div> <div class="links"> </div> <div class="abstract hidden"> <p>Embodiments relate to a human behavior recognition system using hierarchical class learning considering safety, the human behavior recognition system including a behavior class definer configured to form a plurality of behavior classes by sub-setting a plurality of images each including a subject according to pre-designated behaviors and assign a behavior label to the plurality of images, a safety class definer configured to calculate a safety index for the plurality of images, form a plurality of safety classes by sub-setting the plurality of images based on the safety index, and additionally assign a safety label to the plurality of images, and a trainer configured to train a human recognition model by using the plurality of images defined as hierarchical classes by assigning the behavior label and the safety label as training images.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cho2024human</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human behavior recognition system and method using hierachical class learning considering safety}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cho, Junghyun and Kim, Ig Jae and Hwang, Hochul}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{US Patent}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google Patents}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{US Patent App. 17/565,453}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer></footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js">MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>