<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | Hochul Hwang</title> <meta name="author" content="Hochul Hwang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hchlhwang.github.io/SToP/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hochul </span>Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Projects</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"></h1> <p class="post-description"></p> </header> <article> <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"> <title>VisionADL</title> <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> <style>body{font-family:'Open-Sans',sans-serif;font-weight:300;background-color:#fff}.content{width:1000px;padding:25px 50px;margin:25px auto;background-color:white;box-shadow:0 0 10px #999;border-radius:15px}.contentblock{width:950px;margin:0 auto;padding:0;border-spacing:25px 0}.contentblock td{background-color:#fff;padding:25px 50px;vertical-align:top;box-shadow:0 0 10px #999;border-radius:15px}a,a:visited{color:#224b8d;font-weight:300}#authors{text-align:center;margin-bottom:20px}#conference{text-align:center;margin-bottom:20px;font-style:italic}#authors a{margin:0 10px}h1{text-align:center;font-size:35px;font-weight:300}h2{font-size:30px;font-weight:300}code{display:block;padding:10px;margin:10px 10px}p{line-height:25px;text-align:justify}p code{display:inline;padding:0;margin:0}#teasers{margin:0 auto}#teasers td{margin:0 auto;text-align:center;padding:5px}#teasers img{width:250px}#results img{width:133px}#seeintodark{margin:0 auto}#sift{margin:0 auto}#sift img{width:250px}.downloadpaper{padding-left:20px;float:right;text-align:center}.downloadpaper a{font-weight:bold;text-align:center}#demoframe{border:0;padding:0;margin:0;width:100%;height:340px}#feedbackform{border:1px solid #ccc;margin:0 auto;border-radius:15px}#eyeglass{height:530px}#eyeglass #wrapper{position:relative;height:auto;margin:0 auto;float:left;width:800px}#mitnews{font-weight:normal;margin-top:20px;font-size:14px;width:220px}#mitnews a{font-weight:normal}.teaser-img{width:80%;display:block;margin-left:auto;margin-right:auto}.teaser-gif{display:block;margin-left:auto;margin-right:auto}.summary-img{width:100%;display:block;margin-left:auto;margin-right:auto}.video-iframe{width:1000;height:800;margin:auto;display:block}.container{display:flex;align-items:center;justify-content:center}.image{flex-basis:40%}.text{font-size:20px;padding-left:20px}.center{margin-left:auto;margin-right:auto}.boxshadow{border:1px solid;padding:10px;box-shadow:2px 2px 5px #888}.spacertr{height:8px}.spacertd{width:40px}</style> <div class="content"> <h1>Synthetic data augmentation for robotic mobility aids to support blind and low vision people</h1> <p id="authors"> <a href="https://hchlhwang.github.io/">Hochul Hwang</a> <a href="https://www.linkedin.com/in/krisha-adhikari-b0691524b/" rel="external nofollow noopener" target="_blank">Krisha Adhikari</a> <a href="https://www.linkedin.com/in/satyashodhaka/?originalSubdomain=in" rel="external nofollow noopener" target="_blank">Satya Shodhaka</a> <a href="https://www.umass.edu/robotics/people/donghyun-kim" rel="external nofollow noopener" target="_blank">Donghyun Kim</a> <br> DARoS Lab @ UMass Amherst <br><i>RiTA'24</i> </p> <font size="+2"> <p style="text-align: center;"> <a href="https://arxiv.org/pdf/2409.11164" target="_blank" rel="external nofollow noopener">[Paper]</a>      <a href="https://umass-my.sharepoint.com/:f:/g/personal/hochulhwang_umass_edu/EvEzVf7F-zRBgIf-JVVOOeIBKCU_eYts_Ap9NSZdgJo3yg?e=zo1UUI" target="_blank" rel="external nofollow noopener">[Data]</a>      </p> </font> <font size="+1"> </font> <br> <img class="summary-img" src="/assets/img/rita/rita-f1.png" style="width:80%;"> <br> <br> <p id="abstract"><strong>Abstract: </strong> Robotic mobility aids for blind and low-vision (BLV) individuals rely heavily on deep learning-based vision models specialized for various \ navigational tasks. However, the performance of these models is often constrained by the availability and diversity of real-world datasets, which are challenging to collect in sufficient quantities for different tasks. In this study, we investigate the effectiveness of synthetic data, generated using Unreal Engine 4, for training robust vision models for this safety-critical application. Our findings demonstrate that synthetic data can enhance model performance across multiple tasks, showcasing both its potential and its limitations when compared to real-world data. We offer valuable insights into optimizing synthetic data generation for develop- ing robotic mobility aids. Additionally, we publicly release our generated synthetic dataset to support ongoing research in assistive technologies for BLV individuals, available at <a href="https://umass-my.sharepoint.com/:f:/g/personal/hochulhwang_umass_edu/EvEzVf7F-zRBgIf-JVVOOeIBKCU_eYts_Ap9NSZdgJo3yg?e=zo1UUI" target="_blank" rel="external nofollow noopener">[Download]</a> </p> <br clear="all"> </div> <div class="content" id="summary"> <h2 style="text-align:center;">Summary</h2> <p style="text-align: center;">We generated synthetic data using Unreal Engine 4 and the NVIDIA Deep Learning Dataset Synthesizer for various nav- igational downstream tasks. The key contributions of our work can be summarized in threefolds: </p> <ol> <li> We propose a pipeline for generating synthetic data tailored for training deep learning models used in robotic mobility aids.</li> <li> We demonstrate the effectiveness of synthetic data in fine-tuning models for downstream tasks, showing improved performance in tactile paving detection and scene description.</li> <li> We share a comprehensive synthetic dataset that includes a wide range of scenarios, enhancing the robustness of models across various tasks.</li> </ol> <br> <hr> <br> <div class="content2" id="summary"> <h2 style="text-align:center;">Synthetic data generation environment</h2> <p style="text-align: center;">The Suburban environment features urban roads and sidewalks that contain a variety of objects commonly found in sidewalk settings. Controllable camera trajectories allow data collection from diverse viewpoints, reflecting the perspectives of different robotic mobility aids. </p> <img class="summary-img" src="/assets/img/rita/rita-f3.png" style="width:80%;"> <br> <hr> <br> <div class="content3" id="summary"> <h2 style="text-align:center;">Synthetic Tactile-on-Paving (SToP) Dataset</h2> <p style="text-align: center;">Comparison between real data and gen- erated synthetic data in various lighting and viewpoint settings, highlighting the close resemblance of synthetic data to real-world conditions. Visualization of ground truth bounding boxes within the UE4 environment. </p> <img class="summary-img" src="/assets/img/rita/rita-f2.png" style="width:90%;"> <br> <hr> <br> <div class="content3" id="summary"> <h2 style="text-align:center;">Experimental Results 1: Tactile paving detection</h2> <p style="text-align: center;">To evaluate the SToP dataset, we fine-tuned YOLOv8 and YOLO-World specifically for tactile paving detection. </p> <ul> <li>(Left) YOLOv8 successfully detects tactile pavings from a top-down view, which were not detected by the pretrained model with- out synthetic data training.</li> <li>(Right) The open-vocabulary YOLO-World provides bounding boxes for tactile pavings, a capability that was not achieved previously (Center) on a publicly available dataset.</li> <img class="summary-img" src="/assets/img/rita/rita-f4.png" style="width:100%;"> <br> <hr> <br> <div class="content3" id="summary"> <h2 style="text-align:center;">Experimental Results 2: Scene description</h2> <p style="text-align: center;">For the scene description task, we fine-tuned the Florence-2 vision foundation model using our synthetic street crossing dataset, complemented by text annotations crafted by one of the researchers to reflect the informational prefer- ences of BLV individuals during street crossings. As shown in Table 1, incorporating additional real-world and synthetic data led to comparable performance improvements over the baseline, except for precision. Qualitative results of the fine-tuned Florence-2 model are presented in Table 2, where the model exhibited strong performance in generating accurate descriptions across varied conditions, though some inaccuracies and missing information were found. </p> <img class="summary-img" src="/assets/img/rita/rita-f5.png" style="width:80%;"> <br> </div> </ul> </div> </div> </div> </div> </article> </div> </div> <footer></footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js">MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>