<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | Hochul Hwang</title> <meta name="author" content="Hochul Hwang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hchlhwang.github.io/VisionADL/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hochul </span>Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Projects</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"></h1> <p class="post-description"></p> </header> <article> <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"> <title>VisionADL</title> <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> <style>body{font-family:'Open-Sans',sans-serif;font-weight:300;background-color:#fff}.content{width:1000px;padding:25px 50px;margin:25px auto;background-color:white;box-shadow:0 0 10px #999;border-radius:15px}.contentblock{width:950px;margin:0 auto;padding:0;border-spacing:25px 0}.contentblock td{background-color:#fff;padding:25px 50px;vertical-align:top;box-shadow:0 0 10px #999;border-radius:15px}a,a:visited{color:#224b8d;font-weight:300}#authors{text-align:center;margin-bottom:20px}#conference{text-align:center;margin-bottom:20px;font-style:italic}#authors a{margin:0 10px}h1{text-align:center;font-size:35px;font-weight:300}h2{font-size:30px;font-weight:300}code{display:block;padding:10px;margin:10px 10px}p{line-height:25px;text-align:justify}p code{display:inline;padding:0;margin:0}#teasers{margin:0 auto}#teasers td{margin:0 auto;text-align:center;padding:5px}#teasers img{width:250px}#results img{width:133px}#seeintodark{margin:0 auto}#sift{margin:0 auto}#sift img{width:250px}.downloadpaper{padding-left:20px;float:right;text-align:center}.downloadpaper a{font-weight:bold;text-align:center}#demoframe{border:0;padding:0;margin:0;width:100%;height:340px}#feedbackform{border:1px solid #ccc;margin:0 auto;border-radius:15px}#eyeglass{height:530px}#eyeglass #wrapper{position:relative;height:auto;margin:0 auto;float:left;width:800px}#mitnews{font-weight:normal;margin-top:20px;font-size:14px;width:220px}#mitnews a{font-weight:normal}.teaser-img{width:80%;display:block;margin-left:auto;margin-right:auto}.teaser-gif{display:block;margin-left:auto;margin-right:auto}.summary-img{width:100%;display:block;margin-left:auto;margin-right:auto}.video-iframe{width:1000;height:800;margin:auto;display:block}.container{display:flex;align-items:center;justify-content:center}.image{flex-basis:40%}.text{font-size:20px;padding-left:20px}.center{margin-left:auto;margin-right:auto}.boxshadow{border:1px solid;padding:10px;box-shadow:2px 2px 5px #888}.spacertr{height:8px}.spacertd{width:40px}</style> <div class="content"> <h1>VisionADL: Vision-Based Dataset to support activities of daily living for visually impaired individuals</h1> <p id="authors"> <a href="https://hchlhwang.github.io/">Hochul Hwang</a> <a href="https://www.linkedin.com/in/matthew-hersey-a7b9141a1/?trk=people-guest_people_search-card" rel="external nofollow noopener" target="_blank">Matthew Hersey</a> <br> DARoS Lab @ UMass Amherst <br><i>Dataset 2023</i> </p> <font size="+2"> <p style="text-align: center;"> <a href="https://arxiv.org/pdf/2210.13368.pdf" target="_blank" rel="external nofollow noopener">[Paper]</a>      <a href="https://github.com/chail/anyres-gan" target="_blank" rel="external nofollow noopener">[Code]</a>      <a href="/assets/bibliography/visionadl.txt" target="_blank">[Bibtex]</a> </p> </font> <font size="+1"> <p style="text-align: center;"> Skip to:    <a href="#abstract">[Abstract]</a>      <a href="#video">[Supplementary Video]</a>      <a href="#samples">[Random Samples]</a>      </p> </font> <p> <img class="teaser-img" src="/assets/img/visionadl/environment.png"> </p> <br> <p id="abstract"><strong>Abstract: </strong> Generative models operate at fixed resolution, even though natural images come in a variety of sizes. As high-resolution details are downsampled away, and low-resolution images are discarded altogether, precious supervision is lost. We argue that every pixel matters and create datasets with variable-size images, collected at their native resolutions. Taking advantage of this data is challenging; high-resolution processing is costly, and current architectures can only process fixed-resolution data. We introduce continuous-scale training, a process that samples patches at random scales to train a new generator with variable output resolutions. First, conditioning the generator on a target scale allows us to generate higher resolutions images than previously possible, without adding layers to the model. Second, by conditioning on continuous coordinates, we can sample patches that still obey a consistent global layout, allowing for scalable training. Controlled FFHQ experiments show our method takes advantage of the multiresolution training data better than discrete multi-scale approaches, achieving better FID scores and cleaner high-frequency details. We also train on other natural image domains including churches, mountains, and birds, and demonstrate arbitrary scale synthesis with both coherent global layouts and realistic local details, going beyond 2K resolution in our experiments. </p> <br clear="all"> </div> <div class="content" id="summary"> <h2 style="text-align:center;">Summary</h2> <p style="text-align: center;">The typical preprocessing pipeline for unconditional image synthesize resizes all images to the same size, which discards available pixels. We propose a training procedure which can leverage these additional pixels from higher resolution images for image synthesis. </p> <img class="summary-img" src="/assets/img/visionadl/one.jpg" style="width:40%;"> <br> <hr> <p style="text-align: center;">We treat an image as a continuous 2D surface, where real images and synthesized samples correspond to discretizations of this surface. To deal with images of varied sizes, we sample patches of a fixed size at continuous resolutions and locations.</p> <img class="summary-img" src="/assets/img/visionadl/two.jpg" style="width:40%;"> <br> <hr> <p style="text-align: center;">We can modify our approach to synthesize on a cylindrical image plane, which naturally creates 360 degree panoramas. <br>Click <a href="/assets/img/visionadl/blue_bright1.mp4">here</a> to view in video form.</p> <a href="/assets/img/visionadl/blue_dark1.mp4"><img class="summary-img" src="/assets/img/visionadl/blue_dark1.mp4" style="width:30%;"><img class="summary-img" src="/assets/img/visionadl/blue_bright1.mp4" style="width:30%;"></a> <br> </div> <div class="content" id="video"> <h2 style="text-align:center;">Supplementary Video</h2> <p style="text-align: center;">Click <a href="/assets/img/visionadl/blue_dark2.mp4">here</a> to view our supplementary video!</p> <center> <iframe width="600" height="400" src="https://www.youtube.com/embed/9Y7Gvbw0qr4?rel=0&amp;mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </center> </div> <div class="content" id="references"> <h2>Reference</h2> <p>Hwang, H., Xia, T., Keita, I., Suzuki, K., Biswas, J., Lee, S. I., &amp; Kim, D. System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work. ICRA 2023.</p> <code> @article{hwang2022system,<br>   title={System Configuration and Navigation of a Guide Dog Robot: Toward Animal Guide Dog-Level Guiding Work},<br>   author={Hwang, Hochul and Xia, Tim and Keita, Ibrahima and Suzuki, Ken and Biswas, Joydeep and Lee, Sunghoon I and Kim, Donghyun},<br>   journal={arXiv preprint arXiv:2210.13368},<br>   year={2022}<br> } </code> </div> <div class="content" id="acknowledgements"> <p><strong>Acknowledgements</strong>: The website template was adopted from <a href="https://chail.github.io/anyres-gan/" rel="external nofollow noopener" target="_blank">Lucy Chai</a>. </p> </div> </article> </div> </div> <footer></footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js">MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>